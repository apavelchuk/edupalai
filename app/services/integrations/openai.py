import aiohttp
import ssl
from pydantic import BaseModel
import ujson
import certifi

from abc import ABC
from enum import Enum
from functools import wraps
from collections.abc import AsyncIterator

from typing import Awaitable, Optional
from app.config import Config
from app.logger import Logger, logger_factory, log_exec_time
from app.models.content import ContentLanguage, ContentType
from app.services.exceptions import ServiceException
from app.services.service import Service

ssl_context = ssl.create_default_context(cafile=certifi.where())
MAX_TOKENS_PER_REQ = 1000
STREAM_END_MESSAGE = "[DONE]"


class RequestData(BaseModel):
    prompt: str
    max_tokens: Optional[int] = 1000


class RequestMaker(ABC):
    def __init__(self, auth_token: str, logger: Logger):
        self.model: str = self.get_model()
        self.auth_token = auth_token
        self.logger = logger

    def get_model(self):
        raise NotImplementedError()

    def _get_request_headers(self):
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.auth_token}",
        }

    async def generate_content(
        self,
        content_type: ContentType,
        lang: ContentLanguage = ContentLanguage.ENGLISH,
    ) -> Awaitable[str]:
        content = await self.make_request(
            RequestData(prompt=get_autogenerated_content_prompt(content_type, lang)),
        )
        return content

    def make_request(self, data: RequestData) -> str:
        raise NotImplementedError()

    def make_streaming_request(self, data: RequestData) -> AsyncIterator[str]:
        raise NotImplementedError()


class CompletionRequest(RequestMaker):
    URL = "https://api.openai.com/v1/completions"

    async def make_request(self, data: RequestData) -> str:
        json_data = self._get_json_config(data, stream=False)
        async with aiohttp.ClientSession(headers=self._get_request_headers()) as session:
            async with session.post(self.URL, json=json_data, ssl_context=ssl_context) as resp:
                resp_json = await resp.json()
                return resp_json["choices"][0]["text"].strip("\n")

    async def make_streaming_request(self, data: RequestData) -> AsyncIterator[str]:
        json_data = self._get_json_config(data, stream=True)
        async with aiohttp.ClientSession(headers=self._get_request_headers()) as session:
            async with session.post(self.URL, json=json_data, ssl_context=ssl_context) as resp:
                chunk_to_yield = ""
                async for text_chunk, _ in resp.content.iter_chunks():
                    text = self.get_text_from_streaming_chunk(text_chunk)
                    if text:
                        chunk_to_yield += text
                    if text and text.startswith((".", "?", "!")):
                        yield chunk_to_yield
                        chunk_to_yield = ""
                if chunk_to_yield:
                    yield chunk_to_yield

    def _get_json_config(self, data: RequestData, stream: bool) -> dict:
        return {
            "model": self.model,
            "n": 1,
            "best_of": 1,
            "prompt": data.prompt,
            "stream": stream,
            "max_tokens": data.max_tokens,
        }

    def get_model(self) -> str:
        return Config.get("OPENAI_COMPLETIONS_MODEL")

    def get_text_from_streaming_chunk(self, text_chunk: bytes) -> Optional[str]:
        # text_chunk would be like:
        # b'data: {
        # "id": "cmpl-6nlTIiy1lIn5S6rxnjjP0Qx3TBx8H",
        # "object": "text_completion",
        # "created": 1677318572,
        # "choices": [{"text": "!", "index": 0, "logprobs": null, "finish_reason": null}],
        # "model": "text-ada-001"}\n\n'

        try:
            deltas = text_chunk.decode().split("data: ")[1:]
        except Exception:
            raise ServiceException(text_chunk, self.logger)
        if not deltas:
            return None
        try:
            for delta in deltas:
                if delta.startswith(STREAM_END_MESSAGE):
                    return None
                delta_json = ujson.loads(delta)
                top_choice = delta_json["choices"][0]
                if top_choice["finish_reason"] is not None:
                    return None
                if top_choice["text"].strip("\n") == "":
                    return None
                return top_choice["text"]
        except (KeyError, IndexError) as exc:
            raise ServiceException(str(exc), self.logger)


class ChatRequest(RequestMaker):
    URL = "https://api.openai.com/v1/chat/completions"

    def get_model(self) -> str:
        return Config.get("OPENAI_CHAT_MODEL")

    async def make_request(self, data: RequestData) -> str:
        json_data = self._get_json_config(data, stream=False)
        async with aiohttp.ClientSession(headers=self._get_request_headers()) as session:
            async with session.post(self.URL, json=json_data, ssl_context=ssl_context) as resp:
                resp_json = await resp.json()
                return resp_json["choices"][0]["message"]["content"].strip("\n")

    async def make_streaming_request(self, data: RequestData) -> AsyncIterator[str]:
        json_data = self._get_json_config(data, stream=True)
        async with aiohttp.ClientSession(headers=self._get_request_headers()) as session:
            async with session.post(self.URL, json=json_data, ssl_context=ssl_context) as resp:
                chunk_to_yield = ""
                async for text_chunk, _ in resp.content.iter_chunks():
                    text = self.get_text_from_streaming_chunk(text_chunk)
                    if text:
                        chunk_to_yield += text
                    if text and text.startswith((".", "?", "!")):
                        yield chunk_to_yield
                        chunk_to_yield = ""
                if chunk_to_yield:
                    yield chunk_to_yield

    def _get_json_config(self, data: RequestData, stream: bool) -> dict:
        return {
            "model": self.model,
            "n": 1,
            "messages": [{"role": "user", "content": data.prompt}],
            "stream": stream,
            "max_tokens": data.max_tokens,
        }

    def get_text_from_streaming_chunk(self, text_chunk: bytes) -> Optional[str]:
        # text_chunk would be like:
        # b'data: {
        # "id": "cmpl-6nlTIiy1lIn5S6rxnjjP0Qx3TBx8H",
        # "object": "text_completion",
        # "created": 1677318572,
        # "model": "text-ada-001"}
        # "choices": [{"delta": {"content": "\n\n"}, "index": 0,"finish_reason": null}],
        # \n\ndata: {"id":....
        try:
            deltas = text_chunk.decode().split("data: ")[1:]
        except Exception:
            raise ServiceException(text_chunk, self.logger)
        if not deltas:
            return None
        try:
            for delta in deltas:
                if delta.startswith(STREAM_END_MESSAGE):
                    return None
                delta_json = ujson.loads(delta)
                top_choice = delta_json["choices"][0]
                if top_choice["finish_reason"] is not None:
                    return None
                if "content" not in top_choice["delta"] or not top_choice["delta"]["content"].strip("\n"):
                    continue
                return top_choice["delta"]["content"]
        except (KeyError, IndexError, ujson.JSONDecodeError) as exc:
            raise ServiceException(f'delta was: "{delta}", error: {exc}', self.logger)


class ModerationRequest(RequestMaker):
    URL = "https://api.openai.com/v1/moderations"
    MODERATION_FAILED_RESPONSE = "The request is inappropriate. Please try again."

    def __init__(self, actual_req: RequestMaker, *args, **kwargs):
        self.actual_req = actual_req
        super().__init__(*args, **kwargs)

    def get_model(self):
        return self.actual_req.get_model()

    async def make_request(self, data: RequestData) -> str:
        moderation_failed = await self.make_moderation_request(data.prompt)
        if moderation_failed:
            return self.MODERATION_FAILED_RESPONSE
        return await self.actual_req.make_request(data)

    async def make_streaming_request(self, data: RequestData) -> AsyncIterator[str]:
        moderation_failed = await self.make_moderation_request(data.prompt)
        if moderation_failed:
            yield self.MODERATION_FAILED_RESPONSE
            raise StopAsyncIteration()
        async for chunk in self.actual_req.make_streaming_request(data):
            yield chunk

    @log_exec_time("moderation_request")
    async def make_moderation_request(self, prompt: str) -> bool:  # True for inappropriate
        headers = self._get_request_headers()
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.post(self.URL, json={"input": prompt}, ssl_context=ssl_context) as resp:
                resp_json = await resp.json()
        moderation_failed = resp_json["results"][0]["flagged"]
        return moderation_failed


class AI(Service):
    def __init__(self, req_maker: RequestMaker, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_maker = req_maker

    def reply(self, reply_to: str) -> str:
        return self.request_maker.make_request(RequestData(prompt=reply_to))

    def reply_stream(self, reply_to: str) -> AsyncIterator[str]:
        return self.request_maker.make_streaming_request(RequestData(prompt=reply_to))


def openai_service_factory(pre_moderate: Optional[bool] = True) -> AI:
    logger = logger_factory("OpenAI")
    auth_token = Config.get("OPENAI_API_KEY")
    req_maker = CompletionRequest(auth_token=auth_token, logger=logger)
    if pre_moderate:
        req_maker = ModerationRequest(req_maker, auth_token=auth_token, logger=logger)

    return AI(req_maker=req_maker, logger=logger)


class AutogeneratedContentPrompt(Enum):
    TALE = "Generate a short fairy tale for children in 2 sentences in ${lang} language."
    FACT = "Generate a DidYouKnow short fact in 1 sentence in ${lang} language."


def get_autogenerated_content_prompt(content_type: ContentType, lang: ContentLanguage):
    match content_type:
        case ContentType.TALE:
            prompt = AutogeneratedContentPrompt.TALE.value
        case ContentType.FACT:
            prompt = AutogeneratedContentPrompt.FACT.value
        case _:
            raise ServiceException("content_type is not valid.")

    return prompt.replace("${lang}", lang.value)
